{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "datafile = os.path.join('../../..', 'dat/reddit', '2018')\n",
    "\n",
    "with open(datafile, 'r') as f:\n",
    "    record_dicts = []\n",
    "    for line in f.readlines():\n",
    "        record = json.loads(line)\n",
    "        reply_list = record['reply']\n",
    "        earliest_reply_text = None\n",
    "        for reply_dict in sorted(reply_list, key=lambda x: x['created_utc']):\n",
    "            if reply_dict['body'] != '[deleted]' and reply_dict['body'] != '[removed]':\n",
    "                earliest_reply_text = reply_dict['body']\n",
    "            if earliest_reply_text:\n",
    "                break\n",
    "        if earliest_reply_text:\n",
    "            record.pop('reply')\n",
    "            record['reply_text'] = earliest_reply_text\n",
    "            record_dicts.append(record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_df = pd.DataFrame(record_dicts)\n",
    "reddit_df = reddit_df[reddit_df.body != '[deleted]']\n",
    "reddit_df = reddit_df.astype({'score':np.int64, 'controversiality':np.int64, 'gilded':np.int64, 'created_utc':np.int64})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>body</th>\n",
       "      <th>author</th>\n",
       "      <th>author_flair_text</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>link_id</th>\n",
       "      <th>score</th>\n",
       "      <th>controversiality</th>\n",
       "      <th>gilded</th>\n",
       "      <th>id</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>author_flair_css_class</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>reply_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>female</td>\n",
       "      <td>if this isn’t accurate then idk what is</td>\n",
       "      <td>be-concerned</td>\n",
       "      <td>15F</td>\n",
       "      <td>1542407736</td>\n",
       "      <td>t3_9xpaoc</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>e9ufsur</td>\n",
       "      <td>infj</td>\n",
       "      <td>sun</td>\n",
       "      <td>e9ufsur</td>\n",
       "      <td>Yeah, this is definitely me. Then again, I cou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>female</td>\n",
       "      <td>i’m in highschool right now. two years ago, my...</td>\n",
       "      <td>be-concerned</td>\n",
       "      <td>15F</td>\n",
       "      <td>1545673571</td>\n",
       "      <td>t3_a940fq</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>ecgumma</td>\n",
       "      <td>infj</td>\n",
       "      <td>sun</td>\n",
       "      <td>ecgumma</td>\n",
       "      <td>Identify a common interest you have and maybe ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>female</td>\n",
       "      <td>[It’s me.](https://imgur.com/a/WDavpAM)</td>\n",
       "      <td>INFJen</td>\n",
       "      <td>27F</td>\n",
       "      <td>1525914365</td>\n",
       "      <td>t3_8hz688</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>dyqcclj</td>\n",
       "      <td>infj</td>\n",
       "      <td>clover</td>\n",
       "      <td>dyqcclj</td>\n",
       "      <td>Aphrodite? Is that you?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>female</td>\n",
       "      <td>u/User_Simulator u/INFJen</td>\n",
       "      <td>INFJen</td>\n",
       "      <td>27F</td>\n",
       "      <td>1528055310</td>\n",
       "      <td>t3_8o8shk</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>e01ztf1</td>\n",
       "      <td>infj</td>\n",
       "      <td>clover</td>\n",
       "      <td>e01ztf1</td>\n",
       "      <td>+/u/User_Simulator /u/INFJen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>female</td>\n",
       "      <td>It's okay, you too are unique. I'm unique, you...</td>\n",
       "      <td>INFJen</td>\n",
       "      <td>27F</td>\n",
       "      <td>1522538912</td>\n",
       "      <td>t3_88m25u</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>dwlp1jv</td>\n",
       "      <td>infj</td>\n",
       "      <td>clover</td>\n",
       "      <td>dwlp1jv</td>\n",
       "      <td>[Are we?](https://www.wikihow.com/Stop-Taking-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>419014</th>\n",
       "      <td>male</td>\n",
       "      <td>Well, my networth is down something like 150k ...</td>\n",
       "      <td>PowerFIRE</td>\n",
       "      <td>30M, FI@28 with 1M. NW&amp;gt;1.5M. &amp;gt;80% SR. No...</td>\n",
       "      <td>1545837351</td>\n",
       "      <td>t3_a9nqwa</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>ecl84ws</td>\n",
       "      <td>financialindependence</td>\n",
       "      <td></td>\n",
       "      <td>ecl84ws</td>\n",
       "      <td>Oh, sure. Try ERN's article on variable withdr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>419015</th>\n",
       "      <td>male</td>\n",
       "      <td>That's a lot of cash, and doubly so if you are...</td>\n",
       "      <td>PowerFIRE</td>\n",
       "      <td>30M, FI@28 with 1M. NW&amp;gt;1.5M. &amp;gt;80% SR. No...</td>\n",
       "      <td>1544181026</td>\n",
       "      <td>t3_a3wexr</td>\n",
       "      <td>118</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>eba4uha</td>\n",
       "      <td>financialindependence</td>\n",
       "      <td></td>\n",
       "      <td>eba4uha</td>\n",
       "      <td>My 'Quick-math' says 4% of the 2.2 mil he has ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>419016</th>\n",
       "      <td>male</td>\n",
       "      <td>I think it's time for me to buy umbrella insur...</td>\n",
       "      <td>PowerFIRE</td>\n",
       "      <td>30M, FI@28 with 1M. NW&amp;gt;1.5M. &amp;gt;80% SR. No...</td>\n",
       "      <td>1545003232</td>\n",
       "      <td>t3_a6o9qw</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>ebxyzp3</td>\n",
       "      <td>financialindependence</td>\n",
       "      <td></td>\n",
       "      <td>ebxyzp3</td>\n",
       "      <td>I thought it usually goes on top of your homeo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>419017</th>\n",
       "      <td>male</td>\n",
       "      <td>You may get more responses in /r/EuropeFIRE or...</td>\n",
       "      <td>Able_was_I_ERE</td>\n",
       "      <td>����FIREhub.eu | mid-30s M | AT, Europe | Glid...</td>\n",
       "      <td>1530564806</td>\n",
       "      <td>t3_8vi5a5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>e1og0xg</td>\n",
       "      <td>financialindependence</td>\n",
       "      <td></td>\n",
       "      <td>e1og0xg</td>\n",
       "      <td>Thanks man, I will check them out, much apprec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>419018</th>\n",
       "      <td>male</td>\n",
       "      <td>Many places in Vermont, New Hampshire or Maine...</td>\n",
       "      <td>Able_was_I_ERE</td>\n",
       "      <td>����FIREhub.eu | mid-30s M | AT, Europe | Glid...</td>\n",
       "      <td>1518726835</td>\n",
       "      <td>t3_7xt77g</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>duaydje</td>\n",
       "      <td>financialindependence</td>\n",
       "      <td></td>\n",
       "      <td>duaydje</td>\n",
       "      <td>Very specifically NE response\\n\\nEdit: I wonde...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>418670 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        gender                                               body   \n",
       "0       female            if this isn’t accurate then idk what is  \\\n",
       "1       female  i’m in highschool right now. two years ago, my...   \n",
       "2       female           [It’s me.](https://imgur.com/a/WDavpAM)    \n",
       "3       female                          u/User_Simulator u/INFJen   \n",
       "4       female  It's okay, you too are unique. I'm unique, you...   \n",
       "...        ...                                                ...   \n",
       "419014    male  Well, my networth is down something like 150k ...   \n",
       "419015    male  That's a lot of cash, and doubly so if you are...   \n",
       "419016    male  I think it's time for me to buy umbrella insur...   \n",
       "419017    male  You may get more responses in /r/EuropeFIRE or...   \n",
       "419018    male  Many places in Vermont, New Hampshire or Maine...   \n",
       "\n",
       "                author                                  author_flair_text   \n",
       "0         be-concerned                                                15F  \\\n",
       "1         be-concerned                                                15F   \n",
       "2               INFJen                                                27F   \n",
       "3               INFJen                                                27F   \n",
       "4               INFJen                                                27F   \n",
       "...                ...                                                ...   \n",
       "419014       PowerFIRE  30M, FI@28 with 1M. NW&gt;1.5M. &gt;80% SR. No...   \n",
       "419015       PowerFIRE  30M, FI@28 with 1M. NW&gt;1.5M. &gt;80% SR. No...   \n",
       "419016       PowerFIRE  30M, FI@28 with 1M. NW&gt;1.5M. &gt;80% SR. No...   \n",
       "419017  Able_was_I_ERE  ����FIREhub.eu | mid-30s M | AT, Europe | Glid...   \n",
       "419018  Able_was_I_ERE  ����FIREhub.eu | mid-30s M | AT, Europe | Glid...   \n",
       "\n",
       "        created_utc    link_id  score  controversiality  gilded       id   \n",
       "0        1542407736  t3_9xpaoc     60                 0       0  e9ufsur  \\\n",
       "1        1545673571  t3_a940fq      5                 0       0  ecgumma   \n",
       "2        1525914365  t3_8hz688      3                 0       0  dyqcclj   \n",
       "3        1528055310  t3_8o8shk      2                 0       0  e01ztf1   \n",
       "4        1522538912  t3_88m25u      8                 0       0  dwlp1jv   \n",
       "...             ...        ...    ...               ...     ...      ...   \n",
       "419014   1545837351  t3_a9nqwa     22                 0       0  ecl84ws   \n",
       "419015   1544181026  t3_a3wexr    118                 0       0  eba4uha   \n",
       "419016   1545003232  t3_a6o9qw      5                 0       0  ebxyzp3   \n",
       "419017   1530564806  t3_8vi5a5      2                 0       0  e1og0xg   \n",
       "419018   1518726835  t3_7xt77g     13                 0       0  duaydje   \n",
       "\n",
       "                    subreddit author_flair_css_class parent_id   \n",
       "0                        infj                    sun   e9ufsur  \\\n",
       "1                        infj                    sun   ecgumma   \n",
       "2                        infj                 clover   dyqcclj   \n",
       "3                        infj                 clover   e01ztf1   \n",
       "4                        infj                 clover   dwlp1jv   \n",
       "...                       ...                    ...       ...   \n",
       "419014  financialindependence                          ecl84ws   \n",
       "419015  financialindependence                          eba4uha   \n",
       "419016  financialindependence                          ebxyzp3   \n",
       "419017  financialindependence                          e1og0xg   \n",
       "419018  financialindependence                          duaydje   \n",
       "\n",
       "                                               reply_text  \n",
       "0       Yeah, this is definitely me. Then again, I cou...  \n",
       "1       Identify a common interest you have and maybe ...  \n",
       "2                                 Aphrodite? Is that you?  \n",
       "3                            +/u/User_Simulator /u/INFJen  \n",
       "4       [Are we?](https://www.wikihow.com/Stop-Taking-...  \n",
       "...                                                   ...  \n",
       "419014  Oh, sure. Try ERN's article on variable withdr...  \n",
       "419015  My 'Quick-math' says 4% of the 2.2 mil he has ...  \n",
       "419016  I thought it usually goes on top of your homeo...  \n",
       "419017  Thanks man, I will check them out, much apprec...  \n",
       "419018  Very specifically NE response\\n\\nEdit: I wonde...  \n",
       "\n",
       "[418670 rows x 14 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting google-cloud-language\n",
      "  Downloading google_cloud_language-2.17.2-py3-none-any.whl.metadata (9.8 kB)\n",
      "Collecting google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-language)\n",
      "  Downloading google_api_core-2.25.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1 in /Users/enricozhou/.pyenv/versions/3.11.12/lib/python3.11/site-packages (from google-cloud-language) (2.40.3)\n",
      "Collecting proto-plus<2.0.0,>=1.22.3 (from google-cloud-language)\n",
      "  Downloading proto_plus-1.26.1-py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2 in /Users/enricozhou/.pyenv/versions/3.11.12/lib/python3.11/site-packages (from google-cloud-language) (4.25.8)\n",
      "Collecting googleapis-common-protos<2.0.0,>=1.56.2 (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-language)\n",
      "  Downloading googleapis_common_protos-1.70.0-py3-none-any.whl.metadata (9.3 kB)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /Users/enricozhou/.pyenv/versions/3.11.12/lib/python3.11/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-language) (2.31.0)\n",
      "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /Users/enricozhou/.pyenv/versions/3.11.12/lib/python3.11/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-language) (1.73.0)\n",
      "Collecting grpcio-status<2.0.0,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-language)\n",
      "  Downloading grpcio_status-1.73.1-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Users/enricozhou/.pyenv/versions/3.11.12/lib/python3.11/site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-cloud-language) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/enricozhou/.pyenv/versions/3.11.12/lib/python3.11/site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-cloud-language) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/enricozhou/.pyenv/versions/3.11.12/lib/python3.11/site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-cloud-language) (4.9.1)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2 (from google-cloud-language)\n",
      "  Downloading protobuf-6.31.1-cp39-abi3-macosx_10_9_universal2.whl.metadata (593 bytes)\n",
      "Collecting grpcio<2.0.0,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-language)\n",
      "  Using cached grpcio-1.73.1-cp311-cp311-macosx_11_0_universal2.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/enricozhou/.pyenv/versions/3.11.12/lib/python3.11/site-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-language) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/enricozhou/.pyenv/versions/3.11.12/lib/python3.11/site-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-language) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/enricozhou/.pyenv/versions/3.11.12/lib/python3.11/site-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-language) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/enricozhou/.pyenv/versions/3.11.12/lib/python3.11/site-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-language) (2025.6.15)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /Users/enricozhou/.pyenv/versions/3.11.12/lib/python3.11/site-packages (from rsa<5,>=3.1.4->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-cloud-language) (0.6.1)\n",
      "Downloading google_cloud_language-2.17.2-py3-none-any.whl (168 kB)\n",
      "Downloading google_api_core-2.25.1-py3-none-any.whl (160 kB)\n",
      "Downloading googleapis_common_protos-1.70.0-py3-none-any.whl (294 kB)\n",
      "Downloading grpcio_status-1.73.1-py3-none-any.whl (14 kB)\n",
      "Using cached grpcio-1.73.1-cp311-cp311-macosx_11_0_universal2.whl (10.6 MB)\n",
      "Downloading proto_plus-1.26.1-py3-none-any.whl (50 kB)\n",
      "Downloading protobuf-6.31.1-cp39-abi3-macosx_10_9_universal2.whl (425 kB)\n",
      "Installing collected packages: protobuf, grpcio, proto-plus, googleapis-common-protos, grpcio-status, google-api-core, google-cloud-language\n",
      "\u001b[2K  Attempting uninstall: protobuf\n",
      "\u001b[2K    Found existing installation: protobuf 4.25.8\n",
      "\u001b[2K    Uninstalling protobuf-4.25.8:\n",
      "\u001b[2K      Successfully uninstalled protobuf-4.25.8\n",
      "\u001b[2K  Attempting uninstall: grpcio\n",
      "\u001b[2K    Found existing installation: grpcio 1.73.0\n",
      "\u001b[2K    Uninstalling grpcio-1.73.0:m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/7\u001b[0m [grpcio]\n",
      "\u001b[2K      Successfully uninstalled grpcio-1.73.0━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/7\u001b[0m [grpcio]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7/7\u001b[0m [google-cloud-language]le-cloud-language]\n",
      "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow-macos 2.13.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 6.31.1 which is incompatible.\n",
      "tensorflow-macos 2.13.0 requires typing-extensions<4.6.0,>=3.6.6, but you have typing-extensions 4.14.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed google-api-core-2.25.1 google-cloud-language-2.17.2 googleapis-common-protos-1.70.0 grpcio-1.73.1 grpcio-status-1.73.1 proto-plus-1.26.1 protobuf-6.31.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install google-cloud-language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from google.cloud import language\\nfrom google.cloud.language import enums\\nfrom google.cloud.language import types\\nfrom google.cloud import language_v1\\nclient = language_v1.LanguageServiceClient()'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''from google.cloud import language\n",
    "from google.cloud.language import enums\n",
    "from google.cloud.language import types\n",
    "from google.cloud import language_v1\n",
    "client = language_v1.LanguageServiceClient()'''\n",
    "\n",
    "# utilizzato alla fine del file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[     0      1      2 ... 418667 418668 418669]\n",
      "[416107 229372 228260 320597 270002 224328 268956 393887   9607 245145]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "replies = reddit_df[['body','reply_text']].values # mette dentro un array il body e il reply_text separato da una virgola di ogni record, quindi diventa un array di array\n",
    "indices = np.arange(len(replies)) # crea un array di indici da 0 a len(replies)-1\n",
    "print(indices)\n",
    "np.random.shuffle(indices)\n",
    "random_idx = indices[:10]\n",
    "print(random_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['if this isn’t accurate then idk what is',\n",
       "        \"Yeah, this is definitely me. Then again, I could be wrong. I'm usually right but I've been wrong before. You're right I don't know what's true anymore...\"],\n",
       "       ['i’m in highschool right now. two years ago, my friend said that she ships me with this guy i didn’t know. i told her to show me who it is, and when she pointed at him i was amazed. he’s the only person i’ve had a crush on and yet i don’t even talk to him. but hearing him speak to others, watching him play with a little kid, seeing him play instruments- i know exactly what you mean. he’s such a good person at heart and so respectful, he doesn’t judge anyone. but i often see him sitting quietly beside his loud group of friends, earphones in and staring off into space with his head down. i feel like there’s something wrong, but i’m not friends with him and so i don’t know how to talk to him. i hate having a crush on him because i can’t even bring myself to do anything but i can’t help it.',\n",
       "        'Identify a common interest you have and maybe something he knows more about, like an instrument. Say something like: I noticed that you played ____ well. I am just learning it. Do you have time to teach me?\\n\\nMaybe it can be regular thing that you end up spending time together. '],\n",
       "       ['[It’s me.](https://imgur.com/a/WDavpAM) ',\n",
       "        'Aphrodite? Is that you?'],\n",
       "       ...,\n",
       "       [\"I think it's time for me to buy umbrella insurance. Insurance market is so annoying to deal with, though, since you're basically getting the same thing from everybody but for a different price. Has anyone shopped around for this before? What was your method, and who did you end up going with?\",\n",
       "        'I thought it usually goes on top of your homeowners insurance so I would start there.'],\n",
       "       ['You may get more responses in /r/EuropeFIRE or learn more by reading some of these blogs: http://firehub.eu/blogs/country/romania/',\n",
       "        'Thanks man, I will check them out, much appreciated.'],\n",
       "       ['Many places in Vermont, New Hampshire or Maine, e.g. Portland, ME would fit the bill.',\n",
       "        'Very specifically NE response\\n\\nEdit: I wonder what kind of response this post as a whole would have gotten if OP wanted to move to an area that was more conservative?']],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# analizza il sentiment di 10 risposte casuali\\nfor idx in random_idx:\\n    op = replies[idx][0] # il body del post originale\\n    post = replies[idx][1] # il testo della risposta\\n    lines = post.split(\\'\\n\\') # divide il testo della risposta in linee\\n    print(lines)\\n    for text in lines:\\n        if text == \\'\\':\\n            continue\\n        document = types.Document(\\n            content=text,\\n            type=enums.Document.Type.PLAIN_TEXT)\\n        sentiment = client.analyze_sentiment(document=document).document_sentiment\\n        print(\"OP:\", op)\\n        print(\"Text:\", text)\\n        print(\\'Sentiment: {}, {}\\'.format(sentiment.score, sentiment.magnitude))\\n        print(\"*\"*40)'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# non più utilizzato perchè google cloud language non è più gratuito ed types e enums non sono più utilizzati\n",
    "'''# analizza il sentiment di 10 risposte casuali\n",
    "for idx in random_idx:\n",
    "    op = replies[idx][0] # il body del post originale\n",
    "    post = replies[idx][1] # il testo della risposta\n",
    "    lines = post.split('\\n') # divide il testo della risposta in linee\n",
    "    print(lines)\n",
    "    for text in lines:\n",
    "        if text == '':\n",
    "            continue\n",
    "        document = types.Document(\n",
    "            content=text,\n",
    "            type=enums.Document.Type.PLAIN_TEXT)\n",
    "        sentiment = client.analyze_sentiment(document=document).document_sentiment\n",
    "        print(\"OP:\", op)\n",
    "        print(\"Text:\", text)\n",
    "        print('Sentiment: {}, {}'.format(sentiment.score, sentiment.magnitude))\n",
    "        print(\"*\"*40)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Thanks for your response. And yes I do have savings/investments &gt;$300k, and so does my partner. Also, she has a paid-off house. So, financially speaking, my partner would be okay if I were to pass away suddenly.']\n",
      "OP: You don't have debts; but do you have some savings? Worst case scenario, you die on Tuesday (after you've changed this policy on Monday). Is your partner going to be okay? Can she take some time off to mourn? Can she stay in your current house/apartment, at least for a few years until she has the time and energy to move? \n",
      "\n",
      "If you currently already have over $100k in money that will be given to her in a liquid form when you die (so that she can use it to pay rent/mortgage/bills/food/funeral costs/a psychologist when she needs to); then I would do option 1. \n",
      "\n",
      "If you have less than $100k then I would do option 3. And over the next 22 years I would hope that your shared net worth grows to over $100k so that when the term ends, she'll still be good to go if you were to die. \n",
      "Text: Thanks for your response. And yes I do have savings/investments &gt;$300k, and so does my partner. Also, she has a paid-off house. So, financially speaking, my partner would be okay if I were to pass away suddenly.\n",
      "Sentiment: 0.10000000149011612, 1.7999999523162842\n",
      "****************************************\n",
      "['I feel this on a spiritual level. ', '', 'So proud of you, though. Try to send your old self some love, if you can. They deserve it. So do you.']\n",
      "OP: I've been doing pretty well and hit a big goal last week - 100 lbs lost. My tantrum today is that I really hate seeing old photos of myself from my twenties when I was really fat. I try to remember the happy memories and feelings associated with those pictures, but I just feel so much disgust looking at myself. I kind of want to delete them from Facebook and just back them up elsewhere, but I do like seeing the old comments on the photos from friends and family and don't want to lose them. \n",
      "Text: I feel this on a spiritual level. \n",
      "Sentiment: 0.8999999761581421, 0.8999999761581421\n",
      "****************************************\n",
      "OP: I've been doing pretty well and hit a big goal last week - 100 lbs lost. My tantrum today is that I really hate seeing old photos of myself from my twenties when I was really fat. I try to remember the happy memories and feelings associated with those pictures, but I just feel so much disgust looking at myself. I kind of want to delete them from Facebook and just back them up elsewhere, but I do like seeing the old comments on the photos from friends and family and don't want to lose them. \n",
      "Text: So proud of you, though. Try to send your old self some love, if you can. They deserve it. So do you.\n",
      "Sentiment: 0.4000000059604645, 1.7999999523162842\n",
      "****************************************\n",
      "[\"oooh girl we have the same stats! Keep at it. We'll get there together!\"]\n",
      "OP: This is my life at the moment too. Have been floating around the 133-136lbs zone for about two months. Dropped to 131.8 for one day about 2 weeks ago and then went up to 135lbs and now back at 133lbs. Hoping for a whoosh but period is due in less than a week and recently my only whooshes have been after my period has ended. So annoying, and I'm getting so close to my goal!\n",
      "Text: oooh girl we have the same stats! Keep at it. We'll get there together!\n",
      "Sentiment: 0.5, 1.600000023841858\n",
      "****************************************\n",
      "[\"I should say this because it's true and it also tells a person to mind their fucking business. \", '', \"Also, I'm sorry you're going through it. It's a hard pill to swallow. I empathize and here if you need a friend. :)\"]\n",
      "OP: #\"I CAN'T FUCKING GET PREGNANT\"\n",
      "Text: I should say this because it's true and it also tells a person to mind their fucking business. \n",
      "Sentiment: -0.800000011920929, 0.800000011920929\n",
      "****************************************\n",
      "OP: #\"I CAN'T FUCKING GET PREGNANT\"\n",
      "Text: Also, I'm sorry you're going through it. It's a hard pill to swallow. I empathize and here if you need a friend. :)\n",
      "Sentiment: -0.20000000298023224, 2.4000000953674316\n",
      "****************************************\n",
      "['hey! you should date bougie consultant guy!']\n",
      "OP: Pod Save America, NPR Political Podcast, Freakonomics and This American Life\n",
      "Text: hey! you should date bougie consultant guy!\n",
      "Sentiment: 0.4000000059604645, 0.800000011920929\n",
      "****************************************\n",
      "[\"I think these types of things are better shared with a professional. Obviously many people are here in an attempt to better themselves, and that's okay. However, I personally don't believe whatever you look like right now should be so harmful to the point where it completely ruins your day by simply seeing yourself in the mirror. Everyone starts somewhere. You should be proud of the progress you've made, and continually striving to do better, regardless what the mirror shows. \"]\n",
      "OP: At the risk of over sharing...\n",
      "\n",
      "Has anyone else considered covering their bathroom mirrors?  It seems like a not really productive thing to do, but frankly any time I accidentally see myself in them naked I get so disgusted and dejected.  I can have a day where I am proud of my progress and instantly it sends me into a funk.\n",
      "Text: I think these types of things are better shared with a professional. Obviously many people are here in an attempt to better themselves, and that's okay. However, I personally don't believe whatever you look like right now should be so harmful to the point where it completely ruins your day by simply seeing yourself in the mirror. Everyone starts somewhere. You should be proud of the progress you've made, and continually striving to do better, regardless what the mirror shows. \n",
      "Sentiment: 0.0, 2.4000000953674316\n",
      "****************************************\n",
      "['Thank you! Haha, I’ve never been told I look Brazilian before. ']\n",
      "OP: Congratulations! You two look oddly familiar, can't say why exactly. Might be because you look Brazilian. \n",
      "Text: Thank you! Haha, I’ve never been told I look Brazilian before. \n",
      "Sentiment: 0.4000000059604645, 0.8999999761581421\n",
      "****************************************\n",
      "['“You know what I like about these HS girls...”']\n",
      "OP: That Matthew McConaughey quote (you know the one) gets more and more true each year.  \n",
      "Text: “You know what I like about these HS girls...”\n",
      "Sentiment: 0.699999988079071, 0.699999988079071\n",
      "****************************************\n",
      "['I always glance at stats quickly when I see someone\\'s comments just to give \\'em a \"great job/keep going!\" kinda thing, even if they\\'re not OP.', '', \"This time? Oh, yeah. That's u/sassytaters. Of course. Already mentioned it. :)\"]\n",
      "OP: I bet you look great.  Post some progress pics for us!\n",
      "\n",
      "Are you lifting weights?\n",
      "Text: I always glance at stats quickly when I see someone's comments just to give 'em a \"great job/keep going!\" kinda thing, even if they're not OP.\n",
      "Sentiment: 0.10000000149011612, 1.5\n",
      "****************************************\n",
      "OP: I bet you look great.  Post some progress pics for us!\n",
      "\n",
      "Are you lifting weights?\n",
      "Text: This time? Oh, yeah. That's u/sassytaters. Of course. Already mentioned it. :)\n",
      "Sentiment: 0.20000000298023224, 1.7999999523162842\n",
      "****************************************\n",
      "[\"Happy I could help! I'm relative newbie so that makes me happy :) \", \"It def was rich and silky and I wasn't hangry for a longggg time ;)\"]\n",
      "OP: Thank you for this! I'm not a fan of quest bars and I  hadn't really researched low carb shake options. I got some of the single serve packets to try different flavors (so far only vanilla milkshake because I opened that by mistake instead of the banana creme tonight) and it tastes great. I even did the avocado thing and it tasted kind of like pudding which is a good thing for me. \n",
      "Text: Happy I could help! I'm relative newbie so that makes me happy :) \n",
      "Sentiment: 0.8999999761581421, 1.899999976158142\n",
      "****************************************\n",
      "OP: Thank you for this! I'm not a fan of quest bars and I  hadn't really researched low carb shake options. I got some of the single serve packets to try different flavors (so far only vanilla milkshake because I opened that by mistake instead of the banana creme tonight) and it tastes great. I even did the avocado thing and it tasted kind of like pudding which is a good thing for me. \n",
      "Text: It def was rich and silky and I wasn't hangry for a longggg time ;)\n",
      "Sentiment: 0.8999999761581421, 0.8999999761581421\n",
      "****************************************\n"
     ]
    }
   ],
   "source": [
    "# collegamento al servizio di Google Cloud Language\n",
    "from google.cloud import language_v1\n",
    "# collegamento al file di credenziali\n",
    "path = \"/Users/enricozhou/Downloads/stage-466613-83d11aff15ea.json\"\n",
    "# utilizza il file di credenziali per autenticarsi\n",
    "client = language_v1.LanguageServiceClient.from_service_account_json(path)\n",
    "\n",
    "# analizza il sentiment di 10 risposte casuali\n",
    "for idx in random_idx:\n",
    "    op = replies[idx][0]   # il body del post originale\n",
    "    post = replies[idx][1] # il testo della risposta\n",
    "    \n",
    "    lines = post.split('\\n')  # divide il testo della risposta in linee\n",
    "    print(lines)\n",
    "    \n",
    "    for text in lines:\n",
    "        if text.strip() == '':\n",
    "            continue\n",
    "        \n",
    "        # ✅ Nuovo modo di creare il Document\n",
    "        document = language_v1.Document(\n",
    "            content=text,\n",
    "            type_=language_v1.Document.Type.PLAIN_TEXT  # NOTA: underscore dopo type_\n",
    "        )\n",
    "        \n",
    "        # ✅ Analisi del sentiment\n",
    "        response = client.analyze_sentiment(document=document)\n",
    "        sentiment = response.document_sentiment\n",
    "        \n",
    "        print(\"OP:\", op)\n",
    "        print(\"Text:\", text)\n",
    "        print(f\"Sentiment: {sentiment.score}, {sentiment.magnitude}\")\n",
    "        print(\"*\" * 40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
